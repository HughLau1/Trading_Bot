{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbeb95e2",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "110e2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras import callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from IPython.display import clear_output\n",
    "import datetime\n",
    "import statistics\n",
    "import time \n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from keras.models import model_from_json\n",
    "import requests\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e78bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babe428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_setup(symbol,data_len,seq_len):\n",
    "    \n",
    "    #get dataset, has 6 columns\n",
    "    end = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "    start = datetime.datetime.strptime(end, '%Y-%m-%d') - datetime.timedelta(days=(data_len/0.463))\n",
    "    orig_dataset = yf.download(symbol,start,end)\n",
    "    \n",
    "    \n",
    "    print('First 5 rows of dataframe:', orig_dataset.head())\n",
    "    print('Last row of dataframe:', orig_dataset.tail(1))\n",
    "    print('original dataset shape:', orig_dataset.shape)\n",
    "\n",
    "    \n",
    "    #get individual columns\n",
    "    close = orig_dataset['Close'].values\n",
    "    print('Close shape:', close.shape)\n",
    "    open_ = orig_dataset['Open'].values\n",
    "    high = orig_dataset['High'].values\n",
    "    low = orig_dataset['Low'].values\n",
    "    \n",
    "    #Normalize Data\n",
    "    dataset,minmax = normalize_data(orig_dataset)\n",
    "    print('dataset shape:', dataset.shape)\n",
    "\n",
    "    #Get rid of last 2 columns, keep first 4 columns, and put into new array called data\n",
    "    cols = dataset.columns.tolist()\n",
    "    data_seq = list()\n",
    "    for i in range(len(cols)):\n",
    "        if cols[i] < 4:\n",
    "            data_seq.append(dataset[cols[i]].values)\n",
    "            data_seq[i] = data_seq[i].reshape((len(data_seq[i]), 1))\n",
    "    print(len(data_seq))\n",
    "    data = hstack(data_seq)\n",
    "    print('data shape:', data.shape)\n",
    "\n",
    "    #split univariate time series into a trainable datasest, as shown in youtube tutorial\n",
    "    n_steps = seq_len\n",
    "    X, y = split_sequences(data, n_steps)\n",
    "    print('X Shape',X.shape)\n",
    "    print('Y shape', y.shape)\n",
    "    n_features = X.shape[2]\n",
    "    print('n_features:', n_features)\n",
    "    n_seq = len(X)\n",
    "    n_steps = seq_len\n",
    "\n",
    "    #reshape array to fit CNN LSTM\n",
    "    X_reshaped = X.reshape((n_seq,1, n_steps, n_features))\n",
    "    print('Reshaped X Shape',X.shape)\n",
    "    print('Y shape', y.shape)\n",
    "    true_y = []\n",
    "    for i in range(len(y)):\n",
    "        true_y.append([y[i][0],y[i][1]])\n",
    "        \n",
    "    print('true_y shape:', len(true_y))\n",
    "    \n",
    "    \n",
    "    return X, X_reshaped, array(true_y),n_features, minmax, data,  n_steps,close,open_,high,low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd7ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps):\n",
    "        X, y = list(), list()\n",
    "        for i in range(len(sequences)):\n",
    "            end_ix = i + n_steps\n",
    "            if end_ix > len(sequences)-1:\n",
    "                break\n",
    "            seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "        return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5673b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(dataset):\n",
    "        cols = dataset.columns.tolist()\n",
    "        col_name = [0]*len(cols)\n",
    "        for i in range(len(cols)):\n",
    "            col_name[i] = i\n",
    "        dataset.columns = col_name\n",
    "        dtypes = dataset.dtypes.tolist()\n",
    "#         orig_answers = dataset[attr_row_predict].values\n",
    "        minmax = list()\n",
    "        for column in dataset:\n",
    "            dataset = dataset.astype({column: 'float32'})\n",
    "        for i in range(len(cols)):\n",
    "            col_values = dataset[col_name[i]]\n",
    "            value_min = min(col_values)\n",
    "            value_max = max(col_values)\n",
    "            minmax.append([value_min, value_max])\n",
    "        for column in dataset:\n",
    "            values = dataset[column].values\n",
    "            for i in range(len(values)):\n",
    "                values[i] = (values[i] - minmax[column][0]) / (minmax[column][1] - minmax[column][0])\n",
    "            dataset[column] = values\n",
    "        dataset[column] = values\n",
    "        return dataset,minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39eb5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,array(true_y),n_features, minmax, data,  n_steps,close,open_,high,low = data_setup('MSFT', 200, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c569f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def environment_setup(X,y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "#         print('Xtrain shape:', X_train.shape)\n",
    "#         print('Xtest shape:', X_test.shape)\n",
    "#         print('ytrain shape:', y_train.shape)\n",
    "#         print('ytest shape:', y_test.shape)\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b0f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_CNNLSTMnetwork(n_steps,n_features,optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "767d0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_VanillaLSTMnetwork(n_steps,n_features,optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31a63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_StackedLSTMnetwork(n_steps,n_features,optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d963fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_BidirectionalLSTMnetwork(n_steps,n_features,optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88ba94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains model and saves it\n",
    "def train_model(symbol, X_train,y_train,model,epochs):\n",
    "    dirx = r'C:\\Users\\hughx\\Downloads'\n",
    "    os.chdir(dirx)\n",
    "    h5= symbol+'_best_model'+'.h5'\n",
    "    checkpoint = callbacks.ModelCheckpoint(h5, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "    earlystop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=epochs * 1/4, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "    callback = [earlystop,checkpoint] \n",
    "    json = symbol +'_best_model'+'.json'\n",
    "    model_json = model.to_json()\n",
    "    with open(json, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=len(X_train)//4, verbose=2,validation_split = 0.3, callbacks = callback)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "525c1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads the our saved model\n",
    "def load_keras_model(symbol,model,loss,optimizer):\n",
    "    dirx = r'C:\\Users\\hughx\\Downloads'\n",
    "    os.chdir(dirx)\n",
    "    json_file = open(symbol+'_best_model'+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics = None)\n",
    "    model.load_weights(symbol+'_best_model'+'.h5')\n",
    "    return model\n",
    "\n",
    "#uses function above to load most recent model, and evaluates our model\n",
    "def evaluation(symbol, exe_time,X_test, y_test,X_train, y_train ,model,optimizer,loss):\n",
    "    model = load_keras_model(symbol,model,loss,optimizer)\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "    eval_test_loss = round(100-(test_loss*100),1)\n",
    "    eval_train_loss = round(100-(train_loss*100),1)\n",
    "    eval_average_loss = round((eval_test_loss + eval_train_loss)/2,1)\n",
    "    print(\"--- Training Report ---\")\n",
    "#     plot_loss(history)\n",
    "    print('Execution time: ',round(exe_time,2),'s')\n",
    "    print('Testing Accuracy:',eval_test_loss,'%')\n",
    "    print('Training Accuracy:',eval_train_loss,'%')\n",
    "    print('Average Network Accuracy:',eval_average_loss,'%')\n",
    "    return model,eval_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd28f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_predict(model,minmax,n_features,n_steps,data,test_loss):\n",
    "    \n",
    "    pred_data = data[-1].reshape((len(data[-1]),1, n_steps, n_features))\n",
    "    print(data[-1].shape)\n",
    "    print(data[-1])\n",
    "    print(pred_data.shape)\n",
    "    print(pred_data)\n",
    "    pred = model.predict(pred_data)[0]\n",
    "    print(pred)\n",
    "    appro_loss = list()\n",
    "    for i in range(len(pred)):\n",
    "        pred[i] = pred[i] * (minmax[i][1] - minmax[i][0]) + minmax[i][0]\n",
    "        appro_loss.append(((100-test_loss)/100) * (minmax[i][1] - minmax[i][0]))\n",
    "    return pred,appro_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302dcf19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b102f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock = 'TWTR'\n",
    "# myX,myY, myFeatures,  myMinMax, myData, my_nsteps, myclose, myopen, myhigh, mylow = data_setup(stock, 400, 5)\n",
    "\n",
    "# myX_train, myX_test, myy_train, myy_test = environment_setup(myX, myY)\n",
    "\n",
    "# myModel = initialize_CNNLSTMnetwork(5, 4, 'adam')\n",
    "# myModel\n",
    "\n",
    "# trainedModel = train_model(stock, myX_train,myy_train,myModel,500)\n",
    "\n",
    "# models, testloss = evaluation(stock, 10, myX_test, myy_test, myX_train, myy_train ,myModel,'adam','mse')\n",
    "\n",
    "# market_predict(models,myMinMax,4,5,myX,testloss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "First 5 rows of dataframe:                  Open       High        Low      Close  Adj Close    Volume\n",
      "Date                                                                       \n",
      "2019-07-29  41.500000  42.220001  40.900002  41.500000  41.500000  21170600\n",
      "2019-07-30  41.080002  41.720001  40.820000  41.000000  41.000000  15670600\n",
      "2019-07-31  41.080002  43.240002  41.009998  42.310001  42.310001  29745300\n",
      "2019-08-01  42.540001  43.480000  41.720001  42.080002  42.080002  22883600\n",
      "2019-08-02  41.820000  43.040001  41.590000  42.849998  42.849998  20617200\n",
      "Last row of dataframe:                  Open       High        Low      Close  Adj Close    Volume\n",
      "Date                                                                       \n",
      "2021-05-05  54.950001  54.950001  53.349998  53.560001  53.560001  11518900\n",
      "original dataset shape: (447, 6)\n",
      "Close shape: (447,)\n",
      "dataset shape: (447, 6)\n",
      "4\n",
      "data shape: (447, 4)\n",
      "X Shape (442, 5, 4)\n",
      "Y shape (442, 4)\n",
      "n_features: 4\n",
      "Reshaped X Shape (442, 5, 4)\n",
      "Y shape (442, 4)\n",
      "true_y shape: 442\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "3/3 - 1s - loss: 0.1342 - val_loss: 0.1484\n",
      "Epoch 2/100\n",
      "3/3 - 0s - loss: 0.1123 - val_loss: 0.1229\n",
      "Epoch 3/100\n",
      "3/3 - 0s - loss: 0.0915 - val_loss: 0.0993\n",
      "Epoch 4/100\n",
      "3/3 - 0s - loss: 0.0735 - val_loss: 0.0771\n",
      "Epoch 5/100\n",
      "3/3 - 0s - loss: 0.0558 - val_loss: 0.0554\n",
      "Epoch 6/100\n",
      "3/3 - 0s - loss: 0.0393 - val_loss: 0.0359\n",
      "Epoch 7/100\n",
      "3/3 - 0s - loss: 0.0244 - val_loss: 0.0203\n",
      "Epoch 8/100\n",
      "3/3 - 0s - loss: 0.0135 - val_loss: 0.0109\n",
      "Epoch 9/100\n",
      "3/3 - 0s - loss: 0.0078 - val_loss: 0.0084\n",
      "Epoch 10/100\n",
      "3/3 - 0s - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 11/100\n",
      "3/3 - 0s - loss: 0.0071 - val_loss: 0.0088\n",
      "Epoch 12/100\n",
      "3/3 - 0s - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 13/100\n",
      "3/3 - 0s - loss: 0.0044 - val_loss: 0.0041\n",
      "Epoch 14/100\n",
      "3/3 - 0s - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 15/100\n",
      "3/3 - 0s - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 16/100\n",
      "3/3 - 0s - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 17/100\n",
      "3/3 - 0s - loss: 0.0030 - val_loss: 0.0036\n",
      "Epoch 18/100\n",
      "3/3 - 0s - loss: 0.0030 - val_loss: 0.0033\n",
      "Epoch 19/100\n",
      "3/3 - 0s - loss: 0.0027 - val_loss: 0.0030\n",
      "Epoch 20/100\n",
      "3/3 - 0s - loss: 0.0025 - val_loss: 0.0029\n",
      "Epoch 21/100\n",
      "3/3 - 0s - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 22/100\n",
      "3/3 - 0s - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 23/100\n",
      "3/3 - 0s - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 24/100\n",
      "3/3 - 0s - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 25/100\n",
      "3/3 - 0s - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 26/100\n",
      "3/3 - 0s - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 27/100\n",
      "3/3 - 0s - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 28/100\n",
      "3/3 - 0s - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 29/100\n",
      "3/3 - 0s - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 30/100\n",
      "3/3 - 0s - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 31/100\n",
      "3/3 - 0s - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 32/100\n",
      "3/3 - 0s - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 33/100\n",
      "3/3 - 0s - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 34/100\n",
      "3/3 - 0s - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 35/100\n",
      "3/3 - 0s - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 36/100\n",
      "3/3 - 0s - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 37/100\n",
      "3/3 - 0s - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 38/100\n",
      "3/3 - 0s - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 39/100\n",
      "3/3 - 0s - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 40/100\n",
      "3/3 - 0s - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 41/100\n",
      "3/3 - 0s - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 42/100\n",
      "3/3 - 0s - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 43/100\n",
      "3/3 - 0s - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 44/100\n",
      "3/3 - 0s - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 45/100\n",
      "3/3 - 0s - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 46/100\n",
      "3/3 - 0s - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 47/100\n",
      "3/3 - 0s - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 48/100\n",
      "3/3 - 0s - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 49/100\n",
      "3/3 - 0s - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 50/100\n",
      "3/3 - 0s - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 51/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 52/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 53/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 54/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 55/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 56/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 57/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 58/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 59/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 60/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 61/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 62/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 63/100\n",
      "3/3 - 0s - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 64/100\n",
      "3/3 - 0s - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 65/100\n",
      "3/3 - 0s - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 66/100\n",
      "3/3 - 0s - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 67/100\n",
      "3/3 - 0s - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 68/100\n",
      "3/3 - 0s - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 69/100\n",
      "3/3 - 0s - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 70/100\n",
      "3/3 - 0s - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 71/100\n",
      "3/3 - 0s - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 72/100\n",
      "3/3 - 0s - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 73/100\n",
      "3/3 - 0s - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 74/100\n",
      "3/3 - 0s - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 75/100\n",
      "3/3 - 0s - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 76/100\n",
      "3/3 - 0s - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 77/100\n",
      "3/3 - 0s - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 78/100\n",
      "3/3 - 0s - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 79/100\n",
      "3/3 - 0s - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 80/100\n",
      "3/3 - 0s - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 81/100\n",
      "3/3 - 0s - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 82/100\n",
      "3/3 - 0s - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 83/100\n",
      "3/3 - 0s - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 84/100\n",
      "3/3 - 0s - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 85/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 86/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 87/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 88/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 89/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 90/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 91/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 92/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 93/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 94/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 95/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 96/100\n",
      "3/3 - 0s - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 97/100\n",
      "3/3 - 0s - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 98/100\n",
      "3/3 - 0s - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 99/100\n",
      "3/3 - 0s - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 100/100\n",
      "3/3 - 0s - loss: 0.0013 - val_loss: 0.0017\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "3/3 - 0s - loss: 0.1434 - val_loss: 0.0990\n",
      "Epoch 2/100\n",
      "3/3 - 0s - loss: 0.1210 - val_loss: 0.0813\n",
      "Epoch 3/100\n",
      "3/3 - 0s - loss: 0.0989 - val_loss: 0.0647\n",
      "Epoch 4/100\n",
      "3/3 - 0s - loss: 0.0799 - val_loss: 0.0488\n"
     ]
    }
   ],
   "source": [
    "stock = 'TWTR'\n",
    "epochs = 100\n",
    "myX, myXreshaped,myY, myFeatures,  myMinMax, myData, my_nsteps, myclose, myopen, myhigh, mylow = data_setup(stock, 300, 5)\n",
    "# print(myclose)\n",
    "plt.plot(myclose)\n",
    "\n",
    "# make train and test sets\n",
    "\n",
    "# train CNN LSTM\n",
    "myX_train, myX_test, myy_train, myy_test = environment_setup(myXreshaped, myY)\n",
    "myCNNLSTMModel = initialize_CNNLSTMnetwork(5, 4, 'adam')         \n",
    "trainedCNNLSTMModel = train_model(stock, myX_train,myy_train,myCNNLSTMModel,epochs)\n",
    "\n",
    "# train Vanilla LSTM\n",
    "myX_train, myX_test, myy_train, myy_test = environment_setup(myX, myY)\n",
    "myVanillaLSTMModel = initialize_VanillaLSTMnetwork(5, 4, 'adam')\n",
    "trainedVanillaLSTMModel = train_model(stock, myX_train,myy_train,myVanillaLSTMModel,epochs)\n",
    "\n",
    "# train stacked LSTM\n",
    "myStackedLSTMModel = initialize_StackedLSTMnetwork(5, 4, 'adam')\n",
    "trainedStackedLSTMModel = train_model(stock, myX_train,myy_train,myStackedLSTMModel,epochs)\n",
    "\n",
    "# train stacked LSTM\n",
    "myBidirectionalLSTMModel = initialize_BidirectionalLSTMnetwork(5, 4, 'adam')\n",
    "trainedBidirectionalLSTMModel = train_model(stock, myX_train,myy_train,myBidirectionalLSTMModel,epochs)\n",
    "\n",
    "\n",
    "def graph_loss(model, name):\n",
    "    print('keys: ', model.history.keys())\n",
    "    # summarize history for loss\n",
    "    # print(trainedModel.history['loss'])\n",
    "    # print(trainedModel.history['val_loss'])\n",
    "    plt.figure()\n",
    "    x = [i for i in range(len(model.history['loss']))]\n",
    "    plt.plot(x, model.history['loss'])\n",
    "    plt.plot(x, model.history['val_loss'])\n",
    "    plt.title(name)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim([0,.4])\n",
    "    # plt.yscale('log')\n",
    "    # plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "graph_loss(trainedCNNLSTMModel, 'CNN LSTM Loss')\n",
    "graph_loss(trainedVanillaLSTMModel, 'Vanilla LSTNM Loss')\n",
    "graph_loss(trainedStackedLSTMModel, 'Stacked LSTNM Loss')\n",
    "graph_loss(trainedBidirectionalLSTMModel, 'Bidirectional LSTNM Loss')\n",
    "\n",
    "\n",
    "    \n",
    "models, testloss = evaluation(stock, 10, myX_test, myy_test, myX_train, myy_train ,myModel,'adam','mse')\n",
    "# only works for CNNLSTM\n",
    "# market_predict(models,myMinMax,4,5,myX,testloss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd7440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
